{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984e9983-492d-4681-881e-8256797b42e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Youtube to transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebbc8b5-9850-4bce-ada9-6e008255998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: youtube-transcript-api in /root/.local/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.8.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57527a03-367a-449a-9f9a-7988e2d16ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Extract video ID from URL\n",
    "video_url = \"https://www.youtube.com/watch?v=7YHjppyKagc\"\n",
    "video_id = video_url.split(\"v=\")[1].split(\"&\")[0]\n",
    "\n",
    "# Fetch transcript\n",
    "transcript = YouTubeTranscriptApi.get_transcript(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5f8d4b-569d-40e2-8012-a2c0926baa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"let's let's get started\", 'start': 0.52, 'duration': 6.2} {'text': 'um welcome', 'start': 4.72, 'duration': 4.36} {'text': \"everybody very happy you're here this is\", 'start': 6.72, 'duration': 4.919} {'text': 'the first talk in', 'start': 9.08, 'duration': 5.639} {'text': 'the uh neuros symbolic AI Journal', 'start': 11.639, 'duration': 6.681} {'text': \"webinar series and um I'm very happy\", 'start': 14.719, 'duration': 5.841} {'text': 'that Frank is here to', 'start': 18.32, 'duration': 5.4} {'text': 'present thanks a lot Frank over to you', 'start': 20.56, 'duration': 4.559} {'text': \"and it's great to see so many of the\", 'start': 23.72, 'duration': 3.84} {'text': 'people that I know uh to be online so', 'start': 25.119, 'duration': 4.56}\n"
     ]
    }
   ],
   "source": [
    "print(*transcript[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2752229f-64f7-4518-a817-060185837ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"let's let's get started\", 'start': 0.52, 'duration': 6.2},\n",
       " {'text': 'um welcome', 'start': 4.72, 'duration': 4.36},\n",
       " {'text': \"everybody very happy you're here this is\",\n",
       "  'start': 6.72,\n",
       "  'duration': 4.919},\n",
       " {'text': 'the first talk in', 'start': 9.08, 'duration': 5.639},\n",
       " {'text': 'the uh neuros symbolic AI Journal',\n",
       "  'start': 11.639,\n",
       "  'duration': 6.681}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9cf047-098b-4758-a392-41f40509a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all transcript text into one string\n",
    "transcript_text = \" \".join([entry['text'].lower() for entry in transcript])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0fa76f-19d0-4ff6-b90a-7c5e1a1f34a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Topics from CSV and its matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a91ccf-63bd-4866-8357-881c25d266f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# --- Step 1: Load Topics from CSV ---\n",
    "topics_df = pd.read_csv(\"topics.csv\")  # Assumes a column with topic names\n",
    "topics = topics_df.iloc[:, 1].dropna().str.lower().tolist()  # Clean and lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0d26f7-0dc5-4f14-846c-4f20f7b2841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Match Topics in Transcript ---\n",
    "matched_topics = [topic for topic in topics if topic in transcript_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91773834-a0e7-4f16-a1c1-6f767a0e012a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probability',\n",
       " 'machine learning',\n",
       " 'ai',\n",
       " 'deep learning',\n",
       " 'knowledge graphs',\n",
       " 'r',\n",
       " 'cities',\n",
       " 'stem',\n",
       " 'neural networks',\n",
       " 'generative ai',\n",
       " 'large language models',\n",
       " 'semantic web',\n",
       " 'space',\n",
       " 'classes',\n",
       " 'properties',\n",
       " 'relationships',\n",
       " 'semantics',\n",
       " 'inference',\n",
       " 'knowledge graph',\n",
       " 'domain',\n",
       " 'transitivity',\n",
       " 'disjointness',\n",
       " 'graph',\n",
       " 'intersection',\n",
       " 'axiom',\n",
       " 'owl',\n",
       " 'class',\n",
       " 'object',\n",
       " 'predicate',\n",
       " 'property',\n",
       " 'subject',\n",
       " 'triple',\n",
       " 'type',\n",
       " 'construct',\n",
       " 'explain',\n",
       " 'schema',\n",
       " 'uri']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bed370-e927-4b8b-948e-ad4a1013946b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Stopword Removal Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3baca96e-5b90-4498-917a-64472a6e78f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /root/.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /root/.local/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /root/.local/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /root/.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef31ed3-6237-4dfb-aa87-a520b15c175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Make sure you download NLTK resources (only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b83399a0-3f76-4e15-bec3-29c1b1a1e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Remove Stop Words ---\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9438c698-5440-4e9c-a3fa-b0ce7e31900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc189c-ad64-473a-bab5-ccec06390312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# topic match from stop word removed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a01a6626-dc63-409f-9f93-cdea90037e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned transcript and topics\n",
    "cleaned_transcript = clean_text(transcript_text)\n",
    "cleaned_topics = [clean_text(topic) for topic in topics if clean_text(topic)]\n",
    "cleaned_matched_topics = [topic for topic in cleaned_topics if topic in cleaned_transcript]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db9bb6a-58dc-4e0b-9a0a-9914ae75e747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probability',\n",
       " 'machine learning',\n",
       " 'ai',\n",
       " 'deep learning',\n",
       " 'knowledge graphs',\n",
       " 'r',\n",
       " 'cities',\n",
       " 'stem',\n",
       " 'neural networks',\n",
       " 'generative ai']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_matched_topics[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf5f62-adff-47ac-b161-95a650750065",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# use KG directly to find cleaned matched topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35ff9bdb-ce58-47eb-9e1c-d2c9a370ece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: SPARQLWrapper in /root/.local/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: rdflib>=6.1.1 in /root/.local/lib/python3.10/site-packages (from SPARQLWrapper) (7.1.4)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /root/.local/lib/python3.10/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /root/.local/lib/python3.10/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfd42cd-0704-468b-b8e2-e59f004edc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "828b7e7d-9ac2-4c63-9981-0a0d71688566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SPARQL Query Setup ---\n",
    "sparql = SPARQLWrapper(\"http://arsenal.cs.wright.edu:3030/currkg/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "PREFIX edu-ont: <https://edugate.cs.wright.edu/lod/ontology/>\n",
    "\n",
    "SELECT ?topic ?topicTitle\n",
    "WHERE {\n",
    "  ?topic a edu-ont:Topic ;\n",
    "         edu-ont:asString ?topicTitle;\n",
    "}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b1d68f4-1c1f-4459-a3c0-a2a640c8a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topic labels\n",
    "topics_raw_kg = [result['topicTitle']['value'].lower() for result in results[\"results\"][\"bindings\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfb7397b-e76b-4684-a202-d40927dd75ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coding',\n",
       " 'python',\n",
       " 'pandas',\n",
       " 'data science careers',\n",
       " 'probability',\n",
       " 'case studies',\n",
       " 'machine learning',\n",
       " 'natural language processing (nlp)',\n",
       " 'math fundamentals',\n",
       " 'statistics']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_raw_kg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "380f2b49-4a8d-4835-b5fc-115f393dd20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coding',\n",
       " 'python',\n",
       " 'pandas',\n",
       " 'data science careers',\n",
       " 'probability',\n",
       " 'case studies',\n",
       " 'machine learning',\n",
       " 'natural language processing nlp',\n",
       " 'math fundamentals',\n",
       " 'statistics']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_cleaned_topics = [clean_text(topic) for topic in topics_raw_kg if clean_text(topic)]\n",
    "kg_cleaned_topics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c6da3ac-87bc-4855-944d-a1333985761b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probability',\n",
       " 'machine learning',\n",
       " 'ai',\n",
       " 'deep learning',\n",
       " 'knowledge graphs',\n",
       " 'r',\n",
       " 'cities',\n",
       " 'stem',\n",
       " 'neural networks',\n",
       " 'generative ai']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_matched_topics = [topic for topic in kg_cleaned_topics if topic in clean_text(transcript_text)]\n",
    "kg_matched_topics[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47426c25-4511-43e6-afe1-69fe4a4af2aa",
   "metadata": {},
   "source": [
    "## Get Exact transcript entitity to match it to temporal extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e673b138-6d78-4a1f-bd65-a8d9628d09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Match against each transcript entry\n",
    "kg_matched_transcripts_entries = []\n",
    "\n",
    "for entry in transcript:\n",
    "    entry_text_cleaned = clean_text(entry['text'])  # Cleaned version of transcript\n",
    "    for original_topic, cleaned_topic in zip(topics, cleaned_topics):\n",
    "        # Create a regex pattern for exact whole word or phrase match\n",
    "        pattern = r'\\b' + re.escape(cleaned_topic) + r'\\b'\n",
    "        if re.search(pattern, entry_text_cleaned):\n",
    "            kg_matched_transcripts_entries.append({\n",
    "                \"matched_topic\": original_topic,\n",
    "                \"transcript_text\": entry['text'],\n",
    "                \"start\": entry['start'],\n",
    "                \"duration\": entry['duration']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a713c16-cfe2-4c9e-8580-470000420a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easy viewing/export\n",
    "import pandas as pd\n",
    "matched_df = pd.DataFrame(kg_matched_transcripts_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75e601e7-93ee-4f8e-b708-d83b020ce64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_topic</th>\n",
       "      <th>transcript_text</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>knowledge graphs</td>\n",
       "      <td>knowledge graphs but that's just an</td>\n",
       "      <td>98.560</td>\n",
       "      <td>3.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>machine learning</td>\n",
       "      <td>of doing machine learning on symbolic</td>\n",
       "      <td>132.000</td>\n",
       "      <td>4.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disjunction</td>\n",
       "      <td>graph in their abstract or title over</td>\n",
       "      <td>140.879</td>\n",
       "      <td>5.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cities</td>\n",
       "      <td>companies and people and cities and</td>\n",
       "      <td>182.599</td>\n",
       "      <td>4.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cities</td>\n",
       "      <td>companies that people work for cities</td>\n",
       "      <td>185.120</td>\n",
       "      <td>4.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>space</td>\n",
       "      <td>dimensional Vector space uh where</td>\n",
       "      <td>201.440</td>\n",
       "      <td>5.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>disjunction</td>\n",
       "      <td>somehow what is similar in the graph</td>\n",
       "      <td>204.799</td>\n",
       "      <td>4.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>space</td>\n",
       "      <td>space um and then the question is well</td>\n",
       "      <td>211.080</td>\n",
       "      <td>7.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>space</td>\n",
       "      <td>vectors in the space in this case a drug</td>\n",
       "      <td>248.760</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>disjunction</td>\n",
       "      <td>representation uh in this case a graph</td>\n",
       "      <td>291.360</td>\n",
       "      <td>4.720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      matched_topic                           transcript_text    start  \\\n",
       "0  knowledge graphs       knowledge graphs but that's just an   98.560   \n",
       "1  machine learning     of doing machine learning on symbolic  132.000   \n",
       "2       disjunction     graph in their abstract or title over  140.879   \n",
       "3            cities       companies and people and cities and  182.599   \n",
       "4            cities     companies that people work for cities  185.120   \n",
       "5             space         dimensional Vector space uh where  201.440   \n",
       "6       disjunction      somehow what is similar in the graph  204.799   \n",
       "7             space    space um and then the question is well  211.080   \n",
       "8             space  vectors in the space in this case a drug  248.760   \n",
       "9       disjunction    representation uh in this case a graph  291.360   \n",
       "\n",
       "   duration  \n",
       "0     3.400  \n",
       "1     4.440  \n",
       "2     5.561  \n",
       "3     4.161  \n",
       "4     4.119  \n",
       "5     5.879  \n",
       "6     4.681  \n",
       "7     7.079  \n",
       "8     4.000  \n",
       "9     4.720  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_df[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
